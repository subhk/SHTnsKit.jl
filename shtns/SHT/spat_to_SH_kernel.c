// This file was automatically generated by 'make' from file 'kernel_spat_to_SH.gen.c'.
// To modify it, please consider modifying kernel_spat_to_SH.gen.c
/*
 * Copyright (c) 2010-2024 Centre National de la Recherche Scientifique.
 * written by Nathanael Schaeffer (CNRS, ISTerre, Grenoble, France).
 * 
 * nathanael.schaeffer@univ-grenoble-alpes.fr
 * 
 * This software is governed by the CeCILL license under French law and
 * abiding by the rules of distribution of free software. You can use,
 * modify and/or redistribute the software under the terms of the CeCILL
 * license as circulated by CEA, CNRS and INRIA at the following URL
 * "http://www.cecill.info".
 * 
 * The fact that you are presently reading this means that you have had
 * knowledge of the CeCILL license and that you accept its terms.
 * 
 */

//////////////////////////////////////////////////

	#if VSIZE2*NWAY > 32
	#error "VSIZE2*NWAY must not exceed 32"
	#endif

	#ifdef HI_LLIM
	#define BASE _an1_hi
	#else
	#define BASE _an1
	#endif

	void GEN3(BASE,NWAY,SUFFIX)(shtns_cfg shtns, double *BrF, cplx *Qlm, const long int llim, const unsigned im)
  {
	// TODO: NW should be larger for SHTNS_ISHIOKA ?, or take a different approach.
	#define NW (NWAY*2)
	// another blocking level in theta (64 or 128 should be good)
	#define NBLK (NWAY*16) //24 //(NWAY*3) //24 //96 // (NWAY*3)  //24
	// LSPAN can be 2, 4, 6 or 8
	#ifdef __AVX512F__
	#define LSPAN 6
	#else
	#define LSPAN 4
	#endif

	double *alm, *al;
	double *wg, *ct, *st;
	long int nk, k, l,m;
	v2d qq[llim+LSPAN] SSE;

	// the SSE macro should align these arrays on vector length.
  #ifndef SHT_AXISYM
	double reori[NLAT_2*4 + VSIZE2*4] SSE;
  #else
	double reori[NLAT_2*2 + (VSIZE2-1)*2] SSE;
  #endif

	nk = NLAT_2;	// copy NLAT_2 to a local variable for faster access (inner loop limit)
	#if _GCC_VEC_
	  nk = ((unsigned) nk+(VSIZE2-1))/VSIZE2;
	#endif
	wg = shtns->wg;		ct = shtns->ct;		st = shtns->st;

	// ACCESS PATTERN
	const int k_inc = shtns->k_stride_a;
	const int m_inc = shtns->m_stride_a;

	if (im == 0)
	{		// im=0 : evrything is REAL
		alm = shtns->alm2;
	//	double r0 = 0.0;
		// compute symmetric and antisymmetric parts. (do not weight here, it is cheaper to weight y0)
	//	SYM_ASYM_M0_Q(BrF, reori, r0)

		double r0 = split_sym_asym_m0_accl0(BrF, reori, NLAT_2, k_inc, wg);
		{	rnd vr0 = vall(r0 * wg[-1]);
			for (int k=nk-1; k>=0; --k) { 	vstor(reori, 2*k, vread(reori, 2*k) - vr0 );	}	// remove mean from even
		}

		Qlm[0] = r0 * alm[0];			// l=0 is done.
		k = 0;
		double* q_ = (double*) qq;
		for (l=0;l<llim;++l) {
			q_[l] = 0.0;
		}
		do {
			al = alm;
			rnd cost[NW], y0[NW], y1[NW];
			rnd rerk[NW], rork[NW];		// help the compiler to cache into registers.
			const int blk_sze = (k+NW <= nk) ? NW : nk-k;		// limit block size to avoid reading garbage as arrays overflow
			if UNLIKELY(blk_sze <= 0) break;		// allows the compiler to assume blk_sze > 0 in the following.
			for (int j=0; j<blk_sze; ++j) {
				cost[j] = vread(ct, k+j);
				y0[j] = vall(al[0]) * vread(wg, k+j);		// weight of Gauss quadrature appears here
				y1[j] =  (vall(al[1])*y0[j]) * cost[j];
				rerk[j] = vread(reori, (k+j)*2);		rork[j] = vread(reori, (k+j)*2+1);		// cache into registers.
			}
			al+=2;	l=1;
			while(l<llim) {
				rnd q0 = vall(0.0);		rnd q1 = vall(0.0);
				for (int j=0; j<blk_sze; ++j) {
					y0[j]  = vall(al[0])*(cost[j]*y1[j]) + y0[j];
					q1 += y1[j] * rork[j];
					y1[j]  = vall(al[1])*(cost[j]*y0[j]) + y1[j];
					q0 += y0[j] * rerk[j];
				}
				vstor2(q_+l-1,0, vread2(q_+l-1, 0) + v2d_reduce(q1, q0) );
				al+=2;	l+=2;
			}
			if (l==llim) {
				rnd q1 = vall(0.0);
				for (int j=0; j<blk_sze; ++j) {
					q1 += y1[j] * rork[j];
				}
				q_[l-1]   += reduce_add(q1);
			}
			k+=NW;
		} while (k < nk);
		al = shtns->glm_analys;
		Qlm[0] *= al[0];
		for (l=1; l<=llim; ++l) {
			double a = al[l];
			Qlm[l] = q_[l-1] * a;
		}
		#ifdef SHT_VAR_LTR
			for (l=llim+1; l<= LMAX; ++l) {
				((v2d*)Qlm)[l] = vdup(0.0);
			}
		#endif
	}
  #ifndef SHT_AXISYM
	else
	{
		const double mpos_scale = shtns->mpos_scale_analys;		// handles real-norm
		m = im*MRES;
		int k0 = shtns->tm[im] / VSIZE2;
		#if VSIZE2 == 1
		k0 = (k0>>1)*2;		// we need an even value.
		#endif
		#ifndef SHTNS_ISHIOKA
		alm = shtns->alm2 + im*(LMAX+3) - (m*(im-1))/2;
		#else
		alm = shtns->clm + im*(2*(LMAX+1) - m+MRES)/2;
		#endif
		// preprocess spatial data for SIMD processing.
		split_north_south_real_imag(BrF + im*m_inc, BrF + (NPHI-im)*m_inc, reori, k0, NLAT, k_inc);

		for (int l=0; l<=llim-m+1; l++) {
			qq[l] = vdup(0.0);
		}
		int k = nk; do {
			k -= NBLK;
			v2d* q = qq;
			rnd y01ct[NBLK][3];		// [0] = y0,  [1] =  y1,  [2] = cos(theta)^2
		#ifdef HI_LLIM
			int ny[NBLK/NWAY];		// exponent to extend double precision range.
			int lnz = llim;			// apriori, we go up to llim.
		#endif
			int imin = (k<k0) ? k0-k-(NWAY-1) : 0;	// the last block may be smaller.
			for (int i=NBLK-NWAY; i>=imin; i-=NWAY) {		// iterate over blocks, start from the right-most (IMPORTANT!)
				rnd y0[NWAY], cost[NWAY];		// should fit in registers
				for (int j=0; j<NWAY; j++) {
					int idx = k+i+j;	if (idx < 0) idx=0;		// don't read below data.
					y0[j] = vall(mpos_scale);
					cost[j] = vread(st, idx);		// sin(theta)
				}
				l=m;
				int ny0 = 0;
		#ifndef HI_LLIM
					do {		// sin(theta)^m
						if (l&1) for (int j=0; j<NWAY; ++j) y0[j] *= cost[j];
						for (int j=0; j<NWAY; ++j) cost[j] *= cost[j];
					} while(l >>= 1);
		#else  /* HI_LLIM */
				cost[NWAY-1] = vxchg_even_odd(cost[NWAY-1]);	// avoid testing zero for grid including poles.
				{	int nsint = 0;
					do {		// sin(theta)^m		(use rescaling to avoid underflow)
						if (l&1) {
							for (int j=NWAY-1; j>=0; --j) y0[j] *= cost[j];
							ny0 += nsint;
							if (vlo(y0[NWAY-1]) < (SHT_ACCURACY+1.0/SHT_SCALE_FACTOR)) {
								ny0--;
								for (int j=NWAY-1; j>=0; --j) y0[j] *= vall(SHT_SCALE_FACTOR);
							}
						}
						for (int j=NWAY-1; j>=0; --j) cost[j] *= cost[j];
						nsint += nsint;
						if (vlo(cost[NWAY-1]) < 1.0/SHT_SCALE_FACTOR) {
							nsint--;
							for (int j=NWAY-1; j>=0; --j) cost[j] *= vall(SHT_SCALE_FACTOR);
						}
					} while(l >>= 1);
				}
		#endif
				rnd y1[NWAY];
				al = alm;
				for (int j=0; j<NWAY; ++j) {
					int idx = k+i+j;	if (idx < 0) idx=0;		// don't read below data.
					cost[j] = vread(ct, idx);		// cos(theta)
					#ifdef HI_LLIM
					if (j==NWAY-1) cost[j] = vxchg_even_odd(cost[j]);
					#endif
					#ifndef SHTNS_ISHIOKA
					//y0[j] *= vall(al[0]);		// al[0] == 1
					y1[j]  = (vall(al[1])*y0[j]) * cost[j];
					#else
					cost[j] *= cost[j];		// cos(theta)^2
					y1[j] = (vall(al[1])*cost[j] + vall(al[0]))*y0[j];
					#endif
				}

				l=m;	al+=2;
		#ifdef HI_LLIM
			  if (ny0<0) {
				while (l<lnz) {		// ylm's are too small and are treated as zero
					#ifndef SHTNS_ISHIOKA
					for (int j=NWAY-1; j>=0; --j) {
						y0[j] = (vall(al[0])*cost[j])*y1[j] + y0[j];
					}
					for (int j=NWAY-1; j>=0; --j) {
						y1[j] = (vall(al[1])*cost[j])*y0[j] + y1[j];
					}
					l+=2;	al+=2;
					#else
					for (int j=NWAY-1; j>=0; --j) {
						rnd tmp = y1[j];
						y1[j] = (vall(al[1])*cost[j] + vall(al[0]))*y1[j] + y0[j];
						y0[j] = tmp;
					}
					l+=2;	al+=2;
					#endif
					if (fabs(vlo(y0[NWAY-1])) > SHT_ACCURACY*SHT_SCALE_FACTOR + 1.0) {		// rescale when value is significant
						for (int j=NWAY-1; j>=0; --j) {
							y0[j] *= vall(1.0/SHT_SCALE_FACTOR);		y1[j] *= vall(1.0/SHT_SCALE_FACTOR);
						}
						if (++ny0 == 0) break;
					}
				}
			  }
				ny[i/NWAY] = ny0;		// store extended exponents
				if ((l > llim) && (ny0<0)) break;	// nothing more to do in this block.
				lnz = l;				// record the minimum l for significant values, over this block, do not go above that afterwards
				y0[NWAY-1] = vxchg_even_odd(y0[NWAY-1]);
				y1[NWAY-1] = vxchg_even_odd(y1[NWAY-1]);
				cost[NWAY-1] = vxchg_even_odd(cost[NWAY-1]);
		#endif
				for (int j=0; j<NWAY; ++j) {	// store other stuff for recurrence.
					y01ct[i+j][0] = y0[j];
					y01ct[i+j][1] = y1[j];
					y01ct[i+j][2] = cost[j];
				}
			}
		#ifdef HI_LLIM
			if (ny[NBLK/NWAY-1] < 0) break;		// no ylm has a significant value in this block, the block is done.
			l = lnz;	// starting point in l for this block
		#endif

		#ifndef SHTNS_ISHIOKA
			al = alm + 2 + (l-m);
			struct {
				rnd ct;
				rnd y[2];
				rnd rer, rei,  ror, roi;
			} x[NBLK];
			q+=(l-m);
			imin = (k0-k > 0) ? k0-k : 0;	// 0 <= imin < NBLK
			for (int i=imin; i<NBLK; ++i) {
				rnd w = vread(wg, k+i);		x[i].ct = y01ct[i][2];
				x[i].y[0] = y01ct[i][0] * w;	x[i].y[1] = y01ct[i][1] * w;		// weight appears here (must be after the previous accuracy loop).
				rnd rnr = vread(reori, (k+i)*4);		rnd rsr = vread(reori, (k+i)*4 +2);		rnd rni = vread(reori, (k+i)*4 +1);		rnd rsi = vread(reori, (k+i)*4 +3);
				x[i].rer = (rnr+rsr);		x[i].rei = (rni+rsi);		x[i].ror = (rnr-rsr);		x[i].roi = (rni-rsi);
			}
			while (l<llim) {	// compute even and odd parts
				unsigned j = imin;
			#ifdef HI_LLIM
				unsigned ii = imin/NWAY;
				while ((ny[ii] < 0) && (ii<NBLK/NWAY)) {		// these are zeros
					const unsigned imax = (ii+1)*NWAY;
					rnd y0;
					do {
						rnd ct = x[j].ct;
						y0 = x[j].y[0];		rnd y1 = x[j].y[1];
						y0 = vall(al[0])*(ct*y1) + y0;
						x[j].y[0] = y0;
						x[j].y[1] = vall(al[1])*(ct*y0) + y1;
					} while (++j < imax);
					//double yt = ((double*) &x[ii*NWAY+NWAY-1].y[0])[VSIZE2-1];		// access last element, as first one can be zero for grids including poles.
					double yt = vlo(vxchg_even_odd(y0));	// vxchg_even_odd avoids first element which can be zero for grids including poles.
					if (fabs(yt) > SHT_ACCURACY*SHT_SCALE_FACTOR + 1.0) {		// rescale when value is significant
						++ny[ii];
						for (int i=0; i<NWAY; ++i) {
							x[ii*NWAY+i].y[0] *= vall(1.0/SHT_SCALE_FACTOR);		x[ii*NWAY+i].y[1] *= vall(1.0/SHT_SCALE_FACTOR);
						}
					}
					++ii;
				}
			#endif
				rnd qq0 = vall(0.0);	// real
				rnd qq1 = vall(0.0);	// imag
				rnd qq2 = vall(0.0);	// real
				rnd qq3 = vall(0.0);	// real
				while (j<NBLK) {
					register rnd y0 = x[j].y[0];
					register rnd y1 = x[j].y[1];
					{
						qq0 += y0 * x[j].rer;		qq1 += y0 * x[j].rei;	// real even, imag even
						qq2 += y1 * x[j].ror;		qq3 += y1 * x[j].roi;	// real even, imag even
					}
					y0 = vall(al[0])*(x[j].ct*y1) + y0;
					x[j].y[0] = y0;
					x[j].y[1] = vall(al[1])*(x[j].ct*y0) + y1;
					++j;
				}
				#if _GCC_VEC_ && __AVX__
				vstor4(q, 0, vread4(q, 0) + v4d_reduce(qq0, qq1, qq2, qq3) );
				#else
				q[0] += v2d_reduce(qq0, qq1);
				q[1] += v2d_reduce(qq2, qq3);
				#endif
				q+=2;
				l+=2;	al+=2;
			}
			if (l==llim) {
				rnd qq0 = vall(0.0);	// real
				rnd qq1 = vall(0.0);	// imag
				for (unsigned j=imin; j<NBLK; ++j) {
					#ifdef HI_LLIM
					if (ny[j/NWAY] == 0)
					#endif
					{
						register rnd y0 = x[j].y[0];
						qq0 += y0 * x[j].rer;		qq1 += y0 * x[j].rei;	// real even, imag even
					}
				}
				q[0] += v2d_reduce(qq0, qq1);
			}
		#else    /* SHTNS_ISHIOKA */
			al = alm + 2 + (l-m);
			struct {
				rnd y[2];
				rnd ct2;
				rnd rer, rei,  ror, roi;
			} x[NBLK];
			q+=(l-m);
			imin = (k0-k > 0) ? k0-k : 0;	// 0 <= imin < NBLK
			for (int i=imin; i<NBLK; ++i) {
				rnd rnr = vread(reori, (k+i)*4);		rnd rsr = vread(reori, (k+i)*4 +2);		rnd rni = vread(reori, (k+i)*4 +1);		rnd rsi = vread(reori, (k+i)*4 +3);
				rnd w = vread(wg, k+i);		rnd wo = w * vread(ct, k+i);
				x[i].y[0] = y01ct[i][0];		x[i].y[1] = y01ct[i][1];		x[i].ct2 = y01ct[i][2];
				x[i].rer = (rnr+rsr)*w;		x[i].rei = (rni+rsi)*w;		x[i].ror = (rnr-rsr)*wo;		x[i].roi = (rni-rsi)*wo;
			}
			while (l<=llim) {	// compute even and odd parts
				unsigned i = imin;
			#ifdef HI_LLIM
				unsigned ii = imin/NWAY;
				while ((ny[ii] < 0) && (ii<NBLK/NWAY)) {		// these are zeros
					const unsigned imax = (ii+1)*NWAY;
					rnd tmp;
					do {
						rnd ct2 = x[i].ct2;
						#if LSPAN==2
							tmp = x[i].y[1];
							x[i].y[1] = (vall(al[1])*ct2 + vall(al[0]))*tmp + x[i].y[0];
						#elif LSPAN==4
							tmp = (vall(al[1])*ct2 + vall(al[0]))*x[i].y[1] + x[i].y[0];
							x[i].y[1] = (vall(al[3])*ct2 + vall(al[2]))*tmp + x[i].y[1];
						#elif LSPAN==6
							rnd y = (vall(al[1])*ct2 + vall(al[0]))*x[i].y[1] + x[i].y[0];
							tmp = (vall(al[3])*ct2 + vall(al[2]))*y + x[i].y[1];
							x[i].y[1] = (vall(al[5])*ct2 + vall(al[4]))*tmp + y;
						#elif LSPAN==8
							rnd y0 = (vall(al[1])*ct2 + vall(al[0]))*x[i].y[1] + x[i].y[0];
							rnd y1 = (vall(al[3])*ct2 + vall(al[2]))*y0 + x[i].y[1];
							tmp = (vall(al[5])*ct2 + vall(al[4]))*y1 + y0;
							x[i].y[1] = (vall(al[7])*ct2 + vall(al[6]))*tmp + y1;
						#endif
						x[i].y[0] = tmp;
					} while (++i < imax);
					//double yt = ((double*) &x[ii*NWAY+NWAY-1].y[0])[VSIZE2-1];		// access last element, as first one can be zero for grids including poles.
					double yt = vlo(vxchg_even_odd(tmp));	// vxchg_even_odd avoids first element which can be zero for grids including poles.
					if (fabs(yt) > SHT_ACCURACY*SHT_SCALE_FACTOR + 1.0) {		// rescale when value is significant
						++ny[ii];
						for (int j=0; j<NWAY; ++j) {
							x[ii*NWAY+j].y[0] *= vall(1.0/SHT_SCALE_FACTOR);		x[ii*NWAY+j].y[1] *= vall(1.0/SHT_SCALE_FACTOR);
						}
					}
					++ii;
				}
			#endif
				rnd ql[2*LSPAN];
				for (int ll=0; ll<2*LSPAN; ll++) {
					ql[ll] = vall(0.0);
				}
				while (i<NBLK) {
					register rnd y;
					//#pragma GCC unroll 4
					for (int ll=0; ll < LSPAN/2; ll++) {
						if (ll<2) {		// LSPAN==2 or 4
							y = x[i].y[ll];
						} else if (ll==2) {		// LSPAN==6
							y = (vall(al[1])*x[i].ct2 + vall(al[0]))*x[i].y[1] + x[i].y[0];
							x[i].y[0] = (vall(al[3])*x[i].ct2 + vall(al[2]))*y + x[i].y[1];
							x[i].y[1] = (vall(al[5])*x[i].ct2 + vall(al[4]))*x[i].y[0] + y;
						} else if (ll==3) {		// LSPAN==8
							y = x[i].y[0];
						}
						ql[4*ll+0] += y * x[i].rer;		ql[4*ll+1] += y * x[i].rei;
						ql[4*ll+2] += y * x[i].ror;		ql[4*ll+3] += y * x[i].roi;
					}
					#if LSPAN==2
						register rnd tmp = x[i].y[1];
						y = (vall(al[1])*x[i].ct2 + vall(al[0]))*tmp + y;
						x[i].y[0] = tmp;		x[i].y[1] = y;
					#elif LSPAN==4
						x[i].y[0] = (vall(al[1])*x[i].ct2 + vall(al[0]))*y + x[i].y[0];
						x[i].y[1] = (vall(al[3])*x[i].ct2 + vall(al[2]))*x[i].y[0] + y;
					#elif LSPAN==8
						register rnd tmp = x[i].y[1];
						y = (vall(al[7])*x[i].ct2 + vall(al[6]))*tmp + y;
						x[i].y[0] = tmp;		x[i].y[1] = y;
					#endif
					++i;
				}
				for (int ll=0; ll<LSPAN/2; ll++) {		// we can overflow, the work arrays are padded.
				  #if _GCC_VEC_ && __AVX__
					vstor4(q, ll,     vread4(q, ll)     + v4d_reduce(ql[4*ll+0], ql[4*ll+1], ql[4*ll+2], ql[4*ll+3]) );
				  #else
					q[2*ll+0] += v2d_reduce(ql[4*ll+0], ql[4*ll+1]);
					q[2*ll+1] += v2d_reduce(ql[4*ll+2], ql[4*ll+3]);
				  #endif
				}
				q+=LSPAN;
				l+=LSPAN;	al+=LSPAN;
			}
		#endif
		} while (k > k0);

		l = (im*(2*LMAX+2 - (m-MRES)))>>1;		// offset for l=m
		v2d *Ql = (v2d*) &Qlm[l];

	#ifndef SHTNS_ISHIOKA
		double* fl = shtns->glm_analys +  im*(LMAX+3) - (m*(im-1))/2;
		for (long l=0; l<=llim-m; ++l) {
			s2d g = vdup(fl[l]);
			Ql[l] = qq[l] * g;
		}
	#else
		// post-processing for recurrence relation of Ishioka
		const double* restrict xlm = shtns->x2lm + 3*im*(2*(LMAX+4) -m+MRES)/4;
		ishioka_to_SH(xlm, qq, llim-m, Ql);
	#endif


		#ifdef SHT_VAR_LTR
			for (long l=llim+1-m; l<=LMAX-m; ++l) {
				Ql[l] = vdup(0.0);
			}
		#endif
	}
  #endif
  }

	#undef BASE
	#undef LSPAN
	#undef NBLK
